# Adam Optimizer Research Project
## About the Project
As part of the Statistical and Machine Learning module in SUTD (https://esd.sutd.edu.sg/courses/40319-statistical-and-machine-learning/), student pairs were tasked to conduct an open research project into any topic related to the contents of the module. My team decided on deepening our understanding on the Adaptive Moment Estimation (Adam) Optimizer. Access https://lululutt.github.io/SML%20Project.html to follow the tutorial.

## Overall Approach
We first familiarized with the mathematical logic explaining how the Adam Optimizer functions. Other algorithms required to fully understand the Adam Optimizer are Gradient Descent, Gradient Descent with Momentum and Root Mean Square Propogation (RMSProp).

## Application
We compared the different optimizers/algorithms using a popular application - handwritten digit recognization. The dataset used was the highly popular MNIST dataset.